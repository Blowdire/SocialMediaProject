{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Objective of the Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "Analisi del sentiment relativo al dibattito sullo _Smart Working_ in Italia e individuazione di eventuali 'influencer' sul tema."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "tweepy.__version__\n",
    "\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from numpy.core.multiarray import result_type\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the keys here\n",
    "consumer_key = 'VPzjkqKl2y1uSTJQvnVqS9e1X' \n",
    "consumer_secret = 'STG2IzVMf65vPGeOvBQyzdeoKBExAr5sIkhOaBeDe2fnIN14vY'\n",
    "access_token = '1508409949835214853-HIyZJ3oT32TijKsdNDhGFZEEQTWwau'\n",
    "access_token_secret = 'uLcs9hUYmLdocxkaSfXo69Gii46TISu5qZj5F6f6fBfnW'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating an OAuthHandler instance. We pass our consumer key and access token which we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass the OAuthHandler instance into the API method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets that contain a specific hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "hashtag = '(\"#smartworking\" OR \"#remotework\" OR \"#lavoroagile\")'\n",
    "\n",
    "list_tweets = []\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search_tweets, q=hashtag, count=100, lang='it').items(10000):\n",
    "  print('entering')\n",
    "  full_text = api.get_status(tweet.id, tweet_mode='extended')._json['full_text']\n",
    "  print(tweet.id)\n",
    "  list_tweets.append([tweet.created_at, tweet.id, full_text, tweet.favorite_count, tweet.retweet_count, tweet.user.screen_name,\n",
    "                      tweet.user.location, tweet.retweeted, tweet.entities['user_mentions'], tweet.entities['hashtags']])\n",
    "\n",
    "# items is the maximum number of tweets to download.\n",
    "# count is the number of tweets to return per page, up to a maximum of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "keywords = '(\"smartworking\" OR \"remotework\" OR \"lavoroagile\")'\n",
    "\n",
    "list_tweets = []\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search_tweets, q=keywords, count=100, lang='it').items(10000):\n",
    "  print('entering')\n",
    "  full_text = api.get_status(tweet.id, tweet_mode='extended')._json['full_text']\n",
    "  print(tweet.id)\n",
    "  list_tweets.append([tweet.created_at, tweet.id, full_text, tweet.favorite_count, tweet.retweet_count, tweet.user.screen_name,\n",
    "                      tweet.user.location, tweet.retweeted, tweet.entities['user_mentions'], tweet.entities['hashtags']])\n",
    "\n",
    "# items is the maximum number of tweets to download.\n",
    "# count is the number of tweets to return per page, up to a maximum of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn list_tweet into a DataFrame changing column names\n",
    "tweets = pd.DataFrame(list_tweets, columns=['date','id','text','like','n_rt','author','location','retweeted','user_mentions','hastags'])\n",
    "tweets.to_csv('../data/SW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/SW.csv')\n",
    "#tweets_loaded = tweets_loaded.drop('Unnamed: 0', axis=1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets.shape)\n",
    "print(tweets.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "tweets.drop_duplicates(subset =\"id\", inplace = True)\n",
    "tweets.reset_index(drop = True, inplace = True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date format\n",
    "day = tweets['date'].dt.day\n",
    "month = tweets['date'].dt.month\n",
    "year = tweets['date'].dt.year\n",
    "\n",
    "date = year.astype(str) + month.astype(str).str.zfill(2) + day.astype(str).str.zfill(2)\n",
    "date = pd.to_datetime(date, format='%Y%m%d')\n",
    "tweets.drop(columns = ['date'], inplace = True)\n",
    "tweets['date'] = date\n",
    "\n",
    "# Reorder columns\n",
    "cols = tweets.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "tweets = tweets[cols].copy()\n",
    "\n",
    "print('Tweet per day:')\n",
    "print()\n",
    "print(tweets.groupby('date').count()['id'])\n",
    "# print()\n",
    "# print()\n",
    "# print('Minimum Tweet ID per day:')\n",
    "# print()\n",
    "# print(tweets.groupby('date').min('id')['id'])\n",
    "# print()\n",
    "# print()\n",
    "# print('Maximum Tweet ID per day:')\n",
    "# print()\n",
    "# print(tweets.groupby('date').max('id')['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the authors of the tweets and their respective frequency\n",
    "freq_authors = tweets['author'].value_counts()\n",
    "freq_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all hashtags from the full text\n",
    "tweets['hashtags_list'] = tweets['text'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "# Extract all mentions from the full text\n",
    "tweets['mentions'] = tweets['text'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "# \\w matches any single letter, number or underscore (same as [a zA Z0 9_])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK - Natural Language ToolKit is a platform for building Python programs to work with human language data. It provides easy to use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # remove numbers and turning words into lower case\n",
    "    data = data.astype(str).str.replace('\\d+','')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # token lemmatization (ex. goes --> go)\n",
    "    def lemmatize_text(text):\n",
    "        return[(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    # remove punctuation\n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[\\w\\s]', '', (word))\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess_data function\n",
    "pre_tweets = preprocess_data(tweets['text'])\n",
    "tweets['text_proc'] = pre_tweets\n",
    "\n",
    "# delete italian stopwords\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "tweets['text_proc'] = tweets['text_proc'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Content Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Sentiment Analysis_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with social media text, we usually want to identify urls, hashtags, smileys as separate objects and do not tokenize it to individual characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['text', 'text_proc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['scores'] = tweets['text'].apply(lambda Tweet:sent_analyzer.polarity_scores(Tweet))\n",
    "tweets['compound'] = tweets['scores'].apply(lambda score_dict:score_dict['compound'])\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEEL-IT: Emotion and Sentiment Classification for the Italian Language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/sentiment-analysis-and-emotion-recognition-in-italian-using-bert-92f5c8fe8a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feel_it import EmotionClassifier, SentimentClassifier\n",
    "\n",
    "sentiment_classifier = SentimentClassifier()\n",
    "emotion_classifier = EmotionClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feel-it-italian-sentiment model performs sentiment analysis on Italian. We fine-tuned the UmBERTo model on our new dataset (i.e., FEEL-IT) obtaining state-of-the-art performances on different benchmark corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_classifier.predict(tweets[\"text\"].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiment = tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets_sentiment['text']\n",
    "li_sent = []\n",
    "for i in range(0, text.shape[0]):\n",
    "  sent = sentiment_classifier.predict([text[i]])\n",
    "  li_sent.append(sent)\n",
    "  if i % 5000 == 0:\n",
    "    print('Riga',i,'su',text.shape[0])\n",
    "\n",
    "tweets_sentiment['sentiment_BERT'] = [item for sublist in li_sent for item in sublist]\n",
    "     \n",
    "positive = []\n",
    "negative = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "ratio = []\n",
    "\n",
    "for line in tweets_sentiment.values:\n",
    "  sent = line[15]\n",
    "\n",
    "  if sent == 'negative':\n",
    "    positive.append(0)\n",
    "    negative.append(1)\n",
    "    ratio.append(-1)\n",
    "  else:\n",
    "    positive.append(1)\n",
    "    negative.append(0)\n",
    "    ratio.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiment['positive'] = positive\n",
    "tweets_sentiment['negative'] = negative\n",
    "tweets_sentiment['ratio'] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiment.to_csv('../data/SW_sentiment.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Emotion Analysis_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing emotions in text is fundamental to get a better sense of how people are talking about something. People can talk about a new event, but positive/negative labels might not be enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feel-it-italian-emotion model performs emotion classification (joy, fear, anger, sadness) on Italian. We fine-tuned the UmBERTo model on our new dataset (i.e., FEEL-IT) obtaining state-of-the-art performances on different benchmark corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotion_classifier.predict(tweets[\"text\"].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion = tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets_emotion['text']\n",
    "li_emotion = []\n",
    "for i in range(0, text.shape[0]):\n",
    "  emotion = emotion_classifier.predict([text[i]])\n",
    "  li_emotion.append(emotion)\n",
    "  if i % 5000 == 0:\n",
    "    print('Riga',i,'su',text.shape[0])\n",
    "\n",
    "tweets_emotion['emotion_BERT'] = [item for sublist in li_emotion for item in sublist]\n",
    "     \n",
    "anger = []\n",
    "joy = []\n",
    "fear = []\n",
    "sadness = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = []\n",
    "joy = []\n",
    "fear = []\n",
    "sadness = []\n",
    "\n",
    "for line in tweets_emotion.values:\n",
    "  emotion = line[15]\n",
    "\n",
    "  if emotion == 'anger':\n",
    "    anger.append(1)\n",
    "    joy.append(0)\n",
    "    fear.append(0)\n",
    "    sadness.append(0)\n",
    "  elif emotion == 'joy':\n",
    "    anger.append(0)\n",
    "    joy.append(1)\n",
    "    fear.append(0)\n",
    "    sadness.append(0)\n",
    "  elif emotion == 'fear':\n",
    "    anger.append(0)\n",
    "    joy.append(0)\n",
    "    fear.append(1)\n",
    "    sadness.append(0)\n",
    "  else:\n",
    "    anger.append(0)\n",
    "    joy.append(0)\n",
    "    fear.append(0)\n",
    "    sadness.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion['anger'] = anger\n",
    "tweets_emotion['joy'] = joy\n",
    "tweets_emotion['fear'] = fear\n",
    "tweets_emotion['sadness'] = sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion.to_csv('../data/SW_emotion.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Network Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of Centrality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63c35ebd809991b35c43443518cb3da8a509d9076f9b0b6af1c8f1f814340e17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
