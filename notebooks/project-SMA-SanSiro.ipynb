{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Objective of the Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "Analisi del sentiment relativo al dibattito sulla demolizione dello stadio di San Siro a Milano e individuazione di eventuali 'influencer' sul tema."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from numpy.core.multiarray import result_type\n",
    "import time\n",
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the keys here\n",
    "consumer_key = 'VPzjkqKl2y1uSTJQvnVqS9e1X' \n",
    "consumer_secret = 'STG2IzVMf65vPGeOvBQyzdeoKBExAr5sIkhOaBeDe2fnIN14vY'\n",
    "access_token = '1508409949835214853-HIyZJ3oT32TijKsdNDhGFZEEQTWwau'\n",
    "access_token_secret = 'uLcs9hUYmLdocxkaSfXo69Gii46TISu5qZj5F6f6fBfnW'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating an OAuthHandler instance. We pass our consumer key and access token which we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass the OAuthHandler instance into the API method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets that contain a specific hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hashtag = \"#SanSiro\"\n",
    "\n",
    "list_tweets = []\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search_tweets, q=hashtag, count=100, lang='it').items(500):\n",
    "  \n",
    "  full_text = api.get_status(tweet.id, tweet_mode='extended')._json['full_text']\n",
    "\n",
    "  list_tweets.append([tweet.created_at, tweet.id, full_text, tweet.favorite_count, tweet.retweet_count, tweet.user.screen_name,\n",
    "  tweet.user.location, tweet.retweeted, tweet.entities['user_mentions'], tweet.entities['hashtags']])                           \n",
    "\n",
    "# items is the maximum number of tweets to download.\n",
    "# count is the number of tweets to return per page, up to a maximum of 100.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Turn list_tweet into a DataFrame changing column names\n",
    "tweets = pd.DataFrame(list_tweets, columns=['date','id','text','like','n_rt','author','location','retweeted', 'user_mentions', 'hastags'])\n",
    "tweets.to_csv('../data/SanSiro.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>like</th>\n",
       "      <th>n_rt</th>\n",
       "      <th>author</th>\n",
       "      <th>location</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-24 09:51:02+00:00</td>\n",
       "      <td>1606588252269318144</td>\n",
       "      <td>RT @paolopileri_: In Lombardia fioccano candid...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>comitatogoccia</td>\n",
       "      <td>Milano quartiere Bovisa</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'screen_name': 'paolopileri_', 'name': 'Paol...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-24 08:00:49+00:00</td>\n",
       "      <td>1606560513927913473</td>\n",
       "      <td>RT @paolopileri_: In Lombardia fioccano candid...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>pa_poz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'screen_name': 'paolopileri_', 'name': 'Paol...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-24 07:48:15+00:00</td>\n",
       "      <td>1606557349434609666</td>\n",
       "      <td>RT @paolopileri_: In Lombardia fioccano candid...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>iggyHT</td>\n",
       "      <td>milano e trallallero</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'screen_name': 'paolopileri_', 'name': 'Paol...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-23 21:05:36+00:00</td>\n",
       "      <td>1606395621958733824</td>\n",
       "      <td>RT @metaanto: Bravo @VittorioSgarbi il PD vuol...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>CedroneBruno</td>\n",
       "      <td>Il paese delle meraviglie</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'screen_name': 'metaanto', 'name': 'antoniet...</td>\n",
       "      <td>[{'text': 'SanSiro', 'indices': [131, 139]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-23 20:50:04+00:00</td>\n",
       "      <td>1606391715438465038</td>\n",
       "      <td>RT @TamminenJuha: Tifosi del FC Internazionale...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>StatusSymbol2</td>\n",
       "      <td>Milano, Italia</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'screen_name': 'TamminenJuha', 'name': 'Juha...</td>\n",
       "      <td>[{'text': 'tifosi', 'indices': [82, 89]}, {'te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date                   id  \\\n",
       "0  2022-12-24 09:51:02+00:00  1606588252269318144   \n",
       "1  2022-12-24 08:00:49+00:00  1606560513927913473   \n",
       "2  2022-12-24 07:48:15+00:00  1606557349434609666   \n",
       "3  2022-12-23 21:05:36+00:00  1606395621958733824   \n",
       "4  2022-12-23 20:50:04+00:00  1606391715438465038   \n",
       "\n",
       "                                                text  like  n_rt  \\\n",
       "0  RT @paolopileri_: In Lombardia fioccano candid...     0     5   \n",
       "1  RT @paolopileri_: In Lombardia fioccano candid...     0     5   \n",
       "2  RT @paolopileri_: In Lombardia fioccano candid...     0     5   \n",
       "3  RT @metaanto: Bravo @VittorioSgarbi il PD vuol...     0     5   \n",
       "4  RT @TamminenJuha: Tifosi del FC Internazionale...     0     4   \n",
       "\n",
       "           author                   location  retweeted  \\\n",
       "0  comitatogoccia    Milano quartiere Bovisa      False   \n",
       "1          pa_poz                        NaN      False   \n",
       "2          iggyHT       milano e trallallero      False   \n",
       "3    CedroneBruno  Il paese delle meraviglie      False   \n",
       "4   StatusSymbol2             Milano, Italia      False   \n",
       "\n",
       "                                       user_mentions  \\\n",
       "0  [{'screen_name': 'paolopileri_', 'name': 'Paol...   \n",
       "1  [{'screen_name': 'paolopileri_', 'name': 'Paol...   \n",
       "2  [{'screen_name': 'paolopileri_', 'name': 'Paol...   \n",
       "3  [{'screen_name': 'metaanto', 'name': 'antoniet...   \n",
       "4  [{'screen_name': 'TamminenJuha', 'name': 'Juha...   \n",
       "\n",
       "                                             hastags  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3       [{'text': 'SanSiro', 'indices': [131, 139]}]  \n",
       "4  [{'text': 'tifosi', 'indices': [82, 89]}, {'te...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets = pd.read_csv('../data/SanSiro.csv')\n",
    "#tweets = tweets.drop('Unnamed: 0', axis=1) \n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'id', 'text', 'like', 'n_rt', 'author', 'location', 'retweeted',\n",
       "       'user_mentions', 'hastags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "tweets.drop_duplicates(subset =\"id\", inplace = True)\n",
    "tweets.reset_index(drop = True, inplace = True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\FC\\Desktop\\uniProjects\\SocialMediaProject\\notebooks\\project-SMA-SanSiro.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/FC/Desktop/uniProjects/SocialMediaProject/notebooks/project-SMA-SanSiro.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Change date format\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/FC/Desktop/uniProjects/SocialMediaProject/notebooks/project-SMA-SanSiro.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m day \u001b[39m=\u001b[39m tweets[\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mdt\u001b[39m.\u001b[39mday\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/FC/Desktop/uniProjects/SocialMediaProject/notebooks/project-SMA-SanSiro.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m month \u001b[39m=\u001b[39m tweets[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mmonth\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/FC/Desktop/uniProjects/SocialMediaProject/notebooks/project-SMA-SanSiro.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m year \u001b[39m=\u001b[39m tweets[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39myear\n",
      "File \u001b[1;32mc:\\Users\\FC\\miniconda3\\envs\\twitter\\lib\\site-packages\\pandas\\core\\generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5576\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5577\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5578\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5579\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5580\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5581\u001b[0m ):\n\u001b[0;32m   5582\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[1;32mc:\\Users\\FC\\miniconda3\\envs\\twitter\\lib\\site-packages\\pandas\\core\\accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[1;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[0;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\FC\\miniconda3\\envs\\twitter\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:509\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39melif\u001b[39;00m is_period_dtype(data\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m    507\u001b[0m     \u001b[39mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 509\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Change date format\n",
    "day = tweets['date'].dt.day\n",
    "month = tweets['date'].dt.month\n",
    "year = tweets['date'].dt.year\n",
    "\n",
    "date = year.astype(str) + month.astype(str).str.zfill(2) + day.astype(str).str.zfill(2)\n",
    "date = pd.to_datetime(date, format='%Y%m%d')\n",
    "tweets.drop(columns = ['date'], inplace = True)\n",
    "tweets['date'] = date\n",
    "\n",
    "# Reorder columns\n",
    "cols = tweets.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "tweets = tweets[cols].copy()\n",
    "\n",
    "print('Tweet per day:')\n",
    "print()\n",
    "print(tweets.groupby('date').count()['id'])\n",
    "# print()\n",
    "# print()\n",
    "# print('Minimum Tweet ID per day:')\n",
    "# print()\n",
    "# print(df_SanSiro.groupby('date').min('id')['id'])\n",
    "# print()\n",
    "# print()\n",
    "# print('Maximum Tweet ID per day:')\n",
    "# print()\n",
    "# print(df_SanSiro.groupby('date').max('id')['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PianetaMilan     10\n",
       "cmdotcom          5\n",
       "sportface2016     4\n",
       "MiTomorrow        4\n",
       "Pradivio          4\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the authors of the tweets and their respective frequency\n",
    "freq_authors = tweets['author'].value_counts()\n",
    "freq_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all hashtags from the full text\n",
    "tweets['hashtags_list'] = tweets['text'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "\n",
    "# Extract all mentions from the full text\n",
    "tweets['mentions'] = tweets['text'].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "# \\w matches any single letter, number or underscore (same as [a zA Z0 9_])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK - Natural Language ToolKit is a platform for building Python programs to work with human language data. It provides easy to use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\FC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\FC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # remove numbers and turning words into lower case\n",
    "    data = data.astype(str).str.replace('\\d+','')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # token lemmatization (ex. goes --> go)\n",
    "    def lemmatize_text(text):\n",
    "        return[(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    # remove punctuation\n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[\\w\\s]', '', (word))\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FC\\AppData\\Local\\Temp\\ipykernel_14404\\3179111472.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.astype(str).str.replace('\\d+','')\n"
     ]
    }
   ],
   "source": [
    "# apply preprocess_data function\n",
    "pre_tweets = preprocess_data(tweets['text'])\n",
    "tweets['text_proc'] = pre_tweets\n",
    "\n",
    "# delete italian stopwords\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "tweets['text_proc'] = tweets['text_proc'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Content Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sentiment Analysis_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with social media text, we usually want to identify urls, hashtags, smileys as separate objects and do not tokenize it to individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Emotion Analysis_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Network Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of Centrality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63c35ebd809991b35c43443518cb3da8a509d9076f9b0b6af1c8f1f814340e17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
